{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "99IcNqG63q2f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import ensemble\n",
    "from xgboost import XGBClassifier\n",
    "import time\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "emy5HT1T3q2h"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XLy3i9aa3q2i",
    "outputId": "4188a633-5b02-41c5-cbd9-1ad4c43be0ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train = pd.read_csv('120000_augmented.csv')\n",
    "# test = pd.read_csv('../public_test_dataset/data')\n",
    "train.isnull().any().sum()\n",
    "# test.isnull().any().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hPBuBlcpA_3"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "testset = []\n",
    "dir_name = 'data/'\n",
    "lists = sorted(os.listdir(dir_name))\n",
    "for idx, img in enumerate(lists):\n",
    "    image_gray = cv2.imread(dir_name+img, cv2.IMREAD_GRAYSCALE)\n",
    "    image = np.array(image_gray.reshape((784,)))\n",
    "    # image = np.array(Image.open(dir_name+img)).reshape((784,))\n",
    "    image = image.tolist()\n",
    "    testset.append(image)\n",
    "testset = np.array(testset)\n",
    "test = pd.DataFrame(testset)\n",
    "test = test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KWjNSFKVe7lT",
    "outputId": "9aa28133-2898-49bb-a865-a9bedf85598a"
   },
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pPJvXgI0e8Rw",
    "outputId": "b4cbcff1-1070-43b4-f465-84a4afb5bdd9"
   },
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy95owsr3q2i"
   },
   "outputs": [],
   "source": [
    "df_train = train.copy()\n",
    "df_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yyYLogAI3q2i"
   },
   "outputs": [],
   "source": [
    "X_train= df_train.drop(['label'],axis = 1)\n",
    "y_train = df_train['label']\n",
    "X_test = df_test\n",
    "# y_test = df_test.drop(['label'],axis = 1)\n",
    "# y_label = df_test['label']\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255.0\n",
    "# y_test /=255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QO0OCJA_eXnl"
   },
   "outputs": [],
   "source": [
    "# X_train과 X_label을 하나의 데이터 프레임으로 합침\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# 데이터 프레임을 섞음\n",
    "df_train = df_train.sample(frac=1, random_state=42)\n",
    "\n",
    "# 섞인 데이터 프레임에서 훈련 데이터와 레이블을 다시 분리\n",
    "X_train = df_train.drop(['label'], axis=1)\n",
    "y_train = df_train['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVfeVhm5PI7y"
   },
   "outputs": [],
   "source": [
    "from skimage.feature import hog\n",
    "hog_params={\n",
    "    'orientations':6,\n",
    "    'pixels_per_cell':(3,3) ,\n",
    "    'cells_per_block':(3,3),\n",
    "    'block_norm':'L2' ,\n",
    "}\n",
    "def apply_hog(images):\n",
    "    result = []\n",
    "    for image in images:\n",
    "        hog_features = hog(image,**hog_params)\n",
    "        result.append(hog_features)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jdm1cfuAT-wS",
    "outputId": "18cfed8b-ca8b-4f4a-a112-6da369537790"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w93Yskv5UyNr"
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_train = X_train.reshape(-1, 28, 28)\n",
    "X_test = np.array(X_test)\n",
    "X_test = X_test.reshape(-1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s_NWdV_kVOLk",
    "outputId": "f07d5f1d-bfc8-4e9a-efb4-2bee15842f2c"
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iqziXcrPL4Y"
   },
   "outputs": [],
   "source": [
    "X_train_hog = apply_hog(X_train)\n",
    "X_test_hog = apply_hog(X_test)\n",
    "X_train_hog = np.array(X_train_hog)\n",
    "X_test_hog = np.array(X_test_hog)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AE7ds0A4Vlko",
    "outputId": "5d379956-c542-49c0-9959-89b4c821285c"
   },
   "outputs": [],
   "source": [
    "print(\"HOG 특성을 적용한 X_train의 형태: \", np.array(X_train_hog).shape)\n",
    "print(\"HOG 특성을 적용한 X_test의 형태: \", np.array(X_test_hog).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "0w0PlJwpVv8n",
    "outputId": "0543f33a-565b-41eb-cfe1-f2b94b0593f1"
   },
   "outputs": [],
   "source": [
    "from skimage.io import imshow\n",
    "from skimage import exposure\n",
    "# 첫 번째 이미지의 HOG 특성 계산 및 시각화\n",
    "image =image = X_train[0].reshape((28, 28))\n",
    "\n",
    "hog_features, hog_image = hog(image,**hog_params, visualize=True,)\n",
    "\n",
    "# 원본 이미지와 HOG 특성 이미지를 출력\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax1.axis('off')\n",
    "ax1.imshow(image, cmap=plt.cm.gray)\n",
    "ax1.set_title('Input image')\n",
    "\n",
    "# Rescale histogram for better display\n",
    "hog_image_rescaled = exposure.rescale_intensity(hog_image, in_range=(0, 10))\n",
    "\n",
    "ax2.axis('off')\n",
    "ax2.imshow(hog_image_rescaled, cmap=plt.cm.gray)\n",
    "ax2.set_title('Histogram of Oriented Gradients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "id": "bpsKEIf3pBAA",
    "outputId": "cc2ef2a0-ae47-40e9-d090-646009022625",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PCA 모델을 생성하고 학습 데이터에 적합시킵니다.\n",
    "pca = PCA().fit(X_train_hog)\n",
    "\n",
    "# 누적 분산을 계산합니다.\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# 누적 분산 그래프를 그립니다.\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(range(len(cumulative_variance)), cumulative_variance)\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ATZMhyPopBAB",
    "outputId": "b31e72d6-6c9c-4bdb-f613-4147de70d594"
   },
   "outputs": [],
   "source": [
    "# 누적 분산이 95% 이상인 최소 주성분의 수를 찾습니다.\n",
    "num_components = np.where(cumulative_variance >= 0.95)[0][0] + 1\n",
    "print(f\"Number of components for 95% variance: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UjPSuUvoyAzc"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UBqdMEFvpFEj"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100)\n",
    "pca.fit(X_train_hog)\n",
    "X_train_pca = pca.transform(X_train_hog)\n",
    "X_test_pca = pca.transform(X_test_hog)\n",
    "\n",
    "X_train_PCA1 = pd.DataFrame(X_train_pca)\n",
    "X_test_PCA1 = pd.DataFrame(X_test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iH2iv21bpI_Y"
   },
   "outputs": [],
   "source": [
    "optimal_c = 8\n",
    "svc = SVC(gamma='scale',kernel='rbf',C=optimal_c )\n",
    "svc.fit(X_train_PCA1,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hAhl7pHdpo5l"
   },
   "outputs": [],
   "source": [
    "svc_train = svc.predict(X_train_PCA1)\n",
    "svc_pred = svc.predict(X_test_PCA1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5bGUyp3Zra_L"
   },
   "outputs": [],
   "source": [
    "# pred = model_load.predict(test)\n",
    "f= open(\"testResult_public_hog_n400_c8.txt\",\"w+\")\n",
    "for idx, y in enumerate(svc_pred):\n",
    "    num_str = str(idx).zfill(5)\n",
    "    f.write(num_str + \" \" + str(int(y)) + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uQ1fouzzpBAF"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.metrics import auc\n",
    "from collections import Counter\n",
    "\n",
    "# testResult_path = sys.argv[1]\n",
    "# label_path = sys.argv[2]\n",
    "\n",
    "testResult_path = 'testResult_public_hog_n400_c8.txt'\n",
    "label_path = 'label.txt'\n",
    "\n",
    "# pred에 해당하는 testResult.txt 파일 읽어오는 부분입니다.\n",
    "with open(testResult_path, 'r') as file1:\n",
    "    preds = file1.readlines()\n",
    "\n",
    "# 정답에 해당하는 label.txt 파일 읽어오는 부분입니다.\n",
    "with open(label_path, 'r') as file2:\n",
    "    labels = file2.readlines()\n",
    "\n",
    "\n",
    "# pred와 label의 클래스값만 리스트로 변환하는 부분입니다.\n",
    "p = np.array([pred.strip().split()[1] for pred in preds])\n",
    "l = np.array([label.strip().split()[1] for label in labels])\n",
    "\n",
    "# pred의 클래스 개수를 count하는 부분입니다.\n",
    "predict_label_count_dict = Counter(p)\n",
    "predict_label_count_dict = dict(sorted(predict_label_count_dict.items()))\n",
    "\n",
    "## mAP 계산하는 부분입니다.\n",
    "AP = []\n",
    "num_class = 10\n",
    "\n",
    "# 모든 클래스에 대해 반복\n",
    "for c, freq in predict_label_count_dict.items() :\n",
    "    TP = 0\n",
    "    FN = 0\n",
    "\n",
    "    temp_precision = []\n",
    "    temp_recall = []\n",
    "\n",
    "    for i in range(len(p)):\n",
    "        # TP, FN 계산\n",
    "        if l[i] == c and p[i] == c :\n",
    "            TP += 1\n",
    "        elif l[i] != c and p[i] == c :\n",
    "            FN += 1\n",
    "\n",
    "        # preciison, recall 계산\n",
    "        if TP+FN != 0:\n",
    "            temp_precision.append(TP/(TP+FN))\n",
    "            temp_recall.append(TP/freq)\n",
    "\n",
    "    # AP 배열에 클래스 각각의 AP value 저장\n",
    "    # auc : preciison-recall curve의 면적 구해줌\n",
    "    print(\"TP :\", TP)\n",
    "    print(\"FN :\", FN)\n",
    "    # print(temp_precision)\n",
    "    # print(temp_recall)\n",
    "    AP.append(auc(temp_recall, temp_precision))\n",
    "\n",
    "mAP = sum(AP) / num_class\n",
    "\n",
    "# 각각의 클래스에 대한 AP와 mAP의 Table 출력 부분입니다.\n",
    "class_name = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
    "table = \"| {:<13} | {:<13} |\\n\".format(\"Class\", \"AP\") + \"|---------------|---------------|\\n\"\n",
    "\n",
    "for c_name, ap in zip(class_name, AP):\n",
    "    table += \"| {:<13} | {:<13.2f} |\\n\".format(c_name, ap)\n",
    "\n",
    "table += \"| {:<13} | {:<13.2f} |\\n\".format(\"mAP\", mAP)\n",
    "test_mAP = mAP\n",
    "print(table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjTiu9tFpBAG"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "from collections import Counter\n",
    "def calculate_mAP(preds,label):\n",
    "    ## mAP calculation\n",
    "    AP = []\n",
    "    num_class = 10\n",
    "    predict_label_count_dict = Counter(preds)\n",
    "    predict_label_count_dict = dict(sorted(predict_label_count_dict.items()))\n",
    "\n",
    "    # For each class\n",
    "    for c, freq in predict_label_count_dict.items() :\n",
    "        TP = 0\n",
    "        FN = 0\n",
    "\n",
    "        temp_precision = []\n",
    "        temp_recall = []\n",
    "\n",
    "        for i in range(len(preds)):\n",
    "            # Calculate TP and FN\n",
    "            if label[i] == c and preds[i] == c :\n",
    "                TP += 1\n",
    "            elif label[i] != c and preds[i] == c :\n",
    "                FN += 1\n",
    "\n",
    "            # Calculate precision and recall\n",
    "            if TP+FN != 0:\n",
    "                temp_precision.append(TP/(TP+FN))\n",
    "                temp_recall.append(TP/freq)\n",
    "\n",
    "        # Save the AP value of each class to AP array\n",
    "        AP.append(auc(temp_recall, temp_precision))\n",
    "\n",
    "    # Calculate mAP\n",
    "    mAP = sum(AP) / num_class\n",
    "\n",
    "    return mAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9EgZDJ1YpzEJ"
   },
   "outputs": [],
   "source": [
    "# train_acc = metrics.accuracy_score(y_train,svc_train)\n",
    "map_train = calculate_mAP(svc_train,y_train)\n",
    "print(\"Train Map score: {}\".format(map_train))\n",
    "print(\"Test Map score: {}\".format(test_mAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIW-Q--wpBAH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
